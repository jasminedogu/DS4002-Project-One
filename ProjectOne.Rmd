---
title: "Project-One-Group-3"
author: "Jasmine Dogu, Brian Wimmer, Christos Chen - Group 3"
date: "01/04/2021"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
    theme: paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(dplyr)
library(ggplot2)
library(plyr)
library(mlbench)
library(GGally)
library(mltools)
library(data.table)
library(caret)
set.seed(03092000)
```

# Research Question and Hypotheses  
**Goal**: Through this project, our group hopes to utilize both the **Support Vector Machine** (SVM) and **Random Forest Classifier** models to predict whether a person is or is not going to get the H1N1 vaccine. These two algorithms were selected for the reasons that are provided below in the "Why Support Vector Machine and Random Forest Classifier" section. 

We will first begin by explaining some background information on the H1N1 virus, the dataset, and the two machine learning algorithms. From there, we will proceed with the model creation and the parameter tuning. Lastly, we will draw conclusions from our findings and discuss future applications. 

**General Question**: Will the SVM or the Random Forest Model Predict the Likelihood of a Person Getting the H1N1 Vaccine Better?

**Null Hypothesis**: The SVM Polynomial Kernel will not outperform the Random Forest Classifier Model with regards to its F1 Score 
**Alternative Hypothesis**: The SVM Polynomial Kernel will outperform the Random Forest Classifier Model with regards to its F1 Score

**Note**: The F1 score was the metric used to primarily evaluate the two models. To determine if the F1 scores are statistically significant, a **t-test** will be utilized. More information about this can be found in the "Why F1 Score" section below.

# Why a Focus on F1 Score  
![Reference 3](f1.png)

Throughout our project, we will be looking at multiple metrics to evaluate the performance of the SVM and Random Forest models. These include **accuracy, precision (specificity), and recall (sensitivity)**. While all of these metrics will play a role in how we tune the models' parameter and determine which model is more successful, to answer our question of which model outperforms the other we will be looking at the **F1 value**. 

**The F1 value is the harmonic mean of the sensitivity and specificity rates**, and it gives a better measure of the incorrectly classified cases than the accuracy metric would. It is scaled from 0 to 1, with 1 being the best. Because there is an imbalanced class distribution in our dataset, we will be analyzing the F1 score as it is a better indicator of which model is performing better. After getting the F1 scores for both the SVM and the Random Forest models, we will be utilizing a t-test to determine if the difference between the F1 scores of the models is due to chance or is statistically significant. This will determine whether a model outperforms the other.

An explanation of why the SVM and Random Forest models were specifically selected for this project will be explained later after some data cleaning. 

# Background on H1N1
The flu is an illness that is caused by the influenza virus, which can lead to various symptoms that include, but are not limited to, high fevers and sore throats. The Swine Flu, in particular, is a novel influenza A (H1N1) virus that emerged in Spring 2009. It is a subtype of the Influenza A Virus and is considered an **orthomyxovirus** that contains the glycoproteins **haemagglutinin** and **neuraminidase**. The Swine Flu was initially detected in the United States, eventually spreading to the rest of the world. It is said to have "contained a blend of flu genes that hadn't been previously seen in animals or people." The Swine Flu, although very similar to the Seasonal Flu, was found to be more contagious, and less people were found to have existing resistance. (Reference 2)


# About the Dataset  
The selected dataset for this project is originally from the National 2009 H1N1 Flu Survey but can also be found on [Driven Data](https://www.drivendata.org/competitions/66/flu-shot-learning/page/210/). 

Before building our machine learning algorithms, having a clear understanding of the data is critical. We must familiarize ourselves with the origin, size, key characteristics, behavior, and type of data. Our dataset is originally composed of **38 columns** and **26,707 observations**. Each observation accounts for one person who responded to the National 2009 H1N1 Flu Survey. There are five general groupings that each feature falls into. These data categories can be found below with a few examples of the types of variables that would belong to it: 

1. Presumptions
- h1n1_concern, h1n1_knowledge
2. Behavioral: public health measure and avoidance strategies
- behavioral_face_mask, behavioral_large_gatherings, behavioral_touch_face
3. Opinion: respondent’s opinion regarding the vaccine
- opinion_h1n1_risk, opinion_h1n1_sick_from_vacc
4. Demographics
- age_group, income_poverty, education, hhs_geo_region
5. Health Considerations: doctor recommendations, conditions
- doctor_recc_h1n1, chronic_med_condition, health_insurance

According to the CDC, initially there were **734,367 landline numbers** that were considered. After being narrowed down due to the unresponsiveness or the age of the respondent, a total of **105,499** respondents that were adults were found eligible. However, only **43.2%** of these individuals completed the interview. It is fair to assume that there is a **non-response bias** in play with the survey. This is important to keep in mind as it can be a source of explanation as to why the data may be imbalanced. It is very likely that individuals who were pro-vaccination were more likely to want to answer the National 2009 H1N1 Flu Survey, therefore, skewing the results. (Reference 1)

With responses from both the **adults and the children**, the CDC states that there are a total of **56,656** people who responded to the National 2009 H1N1 Flu Survey; these were composed of both landline and cellphone interviews. In our dataset, because we only have data for **26,707** respondents, it is safe to assume that the dataset is a sample of the original National 2009 H1N1 Flu Survey. (Reference 1)

Before getting into the details of why the SVM and Random Forest models were selected, we must clean the data and get a better understanding of it.

# Data Cleaning {.tabset}

## Reading in the Dataset
```{r, include=FALSE}
df_features <- read.csv("training_set_features.csv")
df_label <- read.csv("training_set_labels.csv")
df <- merge(df_features,df_label,by="respondent_id")
#head(df)
#sum(nrow(df))
```
The original dataset was provided in two csv files. To begin our analysis, we must first combine the two into one dataframe. This is because one csv file has the features we are interested in while the other has the labels for our observations (whether the individual received the H1N1 and Seasonal Flu Vaccine or not). We matched up each of the rows by the respondent_id, which was a column in both the features and the label csv files. 

## Removing the Seasonal Flu Columns
```{r}
df <- df[, -which(names(df) %in% c("doctor_recc_seasonal","opinion_seas_vacc_effective","opinion_seas_risk","opinion_seas_sick_from_vacc", "seasonal_vaccine"))]
head(df)
```
Given the recent COVID-19 pandemic, our group chose to focus only on the vaccination for H1N1, which is also known as the Swine Flu. We believe that our findings will be interesting in regards to comparing it to COVID-19. Therefore, we removed the five columns that only dealt with the Seasonal Flu.  

H1N1 was still seen as a “new virus”, although it was a different strain of the seasonal flu. Similarly, the COVID-19 virus and pandemic is also new and relatively unknown. We should be able to draw some similarities between skepticism and opinions regarding vaccinations between the two viruses.

## Looking at the Structure of the Data: Type and NA's
```{r}
str(df)
```
Looking at the structure of the dataset, we notice a few things. Initially, we notice that there are a few empty strings in some of the variables, for example employment_occupation and empoloyment_industry, that we want to convert to NA's. This will make it easier for us when we are analyzing the number of missing datapoints we have in order to figure out what variables we may want to consider dropping. Additionally, all of these variables must be converted to factors. This is because of the nature of the survey and types of variables we have present. 

We notice that for certain variables, specifically hhs_geo_region, employment_industry, and employment_occupation, these use a classification defined by the U.S. Dept. of Health and Human Services. Because these variables are encoded for confidentiality  purposes, and the encoding is not provided online, we are unable to make use these variables. Therefore, these columns will be dropped from the dataset.

## Changing the Empty Strings
```{r}
df <- replace(df, df == "", NA)
```
Here, we replaced all rows with empty strings to contain the value "NA" instead. This will come in handy as we proceed with the analysis.

## Looking at Na's and Removing Specific Columns
```{r}
colSums(is.na(df))
```
Looking at the number of NA's we have present in each column, we can see that the health_insurance, employment_industry, and employment_occupation make up the top 3 columns with the highest amount of NA's.

We decided to remove the health_insurance column for a few reasons. It was the main source of NA's within the dataset (12,274), and deleting rows associated with this column would have effectively eliminated nearly half of the dataset. In addition, we did some further research into the financial obligations regarding the H1N1 vaccine. According to the Centers for Disease Control (CDC), the government wanted to avoid any economic obstacles for everyday Americans when it came to obtaining a vaccine. Vaccination providers, such as clinics or drugstores, were not allowed to charge volunteers for the vaccine, as the supplies had already been purchased by the US Government. This allowed us to conclude that health insurance, therefore, would not be as necessary or as big of an obstacle for obtaining the H1N1 vaccine, in comparison to others. Further information can be found [here](https://www.cdc.gov/h1n1flu/vaccination/statelocal/vaccine_financing.htm).

We also chose to remove the employment_industry and employment_occupation variables. Similarly to health_insurance, removing observations with “NA” for these two variables would have resulted in nearly half of our dataset being eliminated. These variables include “codes” that are correlated to US Census data regarding industry and occupation types. Converting these to factor variables and then taking the average or median to engineer data for the NA's would be inaccurate. We would be left with very high numbers of one particular industry and occupation, which would not help the model in determining if people would get an H1N1 vaccine or not.

Lastly, we removed the respondent_id at this step. This variable was important when we were matching the features to the labels. However, for the machine learning algorithm, it is not needed.

```{r}
df <- df[, -which(names(df) %in% c("employment_occupation","health_insurance","employment_industry", "hhs_geo_region", "respondent_id"))]
df<- na.omit(df)
#write.csv(df,"df2.csv", row.names = TRUE) #saving to a new csv file
df2<- read.csv("df2.csv")  
nrow(df)
```
Here, along with removing the top 3 columns with the highest number of NA's, we also removed the hhs_geo_region variable for reasons mentioned above. We are left with 19,656 observations. Because this is still a significant portion of the original number of observations the dataset had (approximately 73.6%), we proceed with our analysis.

In this step of our project, we decided to save the current dataframe to a new csv file. This is because in the next step we will be coercing all of the features to factors; therefore, it is a good idea to keep a copy of the clean data that contains all of the features' data types as they originally were.

## Coercing the Variables to Factors
```{r}
df[,] <- lapply(df[,], factor)  ## as.factor() could also be used
str(df)
#write.csv(df,"cleaned_df.csv", row.names = TRUE) #saving to a new csv file
```
After this step, we can verify that all of the variables are now factors by using the str() function. Because they are all factors, we can carry on with our analysis.

# Why Support Vector Machine and Random Forest Classifier  
```{r, message = FALSE, warnings = FALSE}
library(dplyr)
library(tidyverse)
pie( c( nrow(df %>% filter(h1n1_vaccine==0)), nrow(df %>% filter(h1n1_vaccine==1))), labels = c("Not Vaccinated, 77%", "Got Vaccinated, 23%"),border = "white", col= c("#015977", "#606DA2"), main = "Base Rate of H1N1 Vaccinations" )
```

After a preliminary analysis of the data topology, our group decided to compare the SVM and Random Forest models. We were curious to how these two models would perform against one another due to the nature of the algorithms. Both are **supervised machine learning algorithms** which can be used for **classification** or **regression** analysis. For the purposes of our analysis, we will be using these algorithms to classify which group a person, given various features, is likely to belong to (vaccine/no vaccine). We selected these algorithms because they are both great for imbalanced datasets, which as we can see above, our dataset has a base rate of 23% for the group who received the vaccine; because our dataset is imbalanced, it is critical to utilize algorithms that can deal with it fairly well. Similarly, both of the algorithms reduce the risk of over-fitting in their own manner. This is also extremely important for our model as overfitting to the dataset can falsely skew the accuracy of the model.

In theory, the Random Forest model would have a few benefits over the SVM model. The Random Forest is non-parametric, so outliers would not be an issue for the algorithm. Similarly, they are extremely easy to build (compared to SVMs) and are fast/scalable. Unlike with SVMs, Random Forests do not require a lot of parameter tuning. These are all characteristics of the algorithm that make it extremely favorable for this research study.

However, the SVM model also has a few general benefits over the Random Forest model. Usually, the SVM model is known to be highly efficient and accurate. Similarly, the SVM model scales well to high dimensional data, which is not significantly relevant for our dataset as we have more observations than we do features. However, the main reason why we predict that the SVM model will outperform the Random Forest model is because of the kernels that the SVM model uses. With an appropriate kernel and degree of the kernel, we can expect the algorithm to work extremely well even if the data is not linearly separable in the base feature space, which in this instance applies to our dataset.

In order to begin building our model, we now proceed to the EDA, followed by the feature selection. These are two sections that will help us gain a better understanding of the dataset and re-adjust our hypotheses, if needed. 

# EDA {.tabset}

In order to continue with the project, we first wanted to explore the data and to try to get an idea of the types of people within the dataset that may choose to get the vaccine. For this, we decided to create histograms depicting different features from each of the categories we had: presumptions, behavioral, opinion, demographic, and health considerations. In our EDA, we will mainly be exploring the question of "Who is being represented in the data?" We will then be looking at the various categories to understand the following:

- Presumptions: How concerned and how much knowledge do respondents have regarding the H1N1 virus?

- Behavioral: How have respondents behavior changed in response to the H1N1 virus?

- Opinion: What opinions do the respondents hold regarding H1N1 risk and the vaccine?

- Demographics: What are the demographics of the respondents?

- Health: What health considerations did the respondents report?

## Histogram for "Presumptions" Variables
```{r}
pre_1 <- ggplot(df2, aes(x=h1n1_concern)) +
  geom_histogram(stat="count",bins=4,fill="#015977",color="black")+xlab("Level of H1N1 Concern") + geom_text(aes( label = scales::percent(..prop..),y= ..prop.. ), stat= "count", vjust = -.5)
pre_1
```

Over 70 percent of respondents presented a moderate level of concern regarding H1N1. 11.5% and 16.2% indicated "not at all concerned" and "very concerned", respectively. We would expect those that indicated "very concerned" to be the most likely to get the H1N1 vaccine.

0-Not at all concerned  
1-Not very concerned  
2-Somewhat concerned  
3-Very concerned  

```{r}
pre_2 <- ggplot(df2, aes(x=h1n1_knowledge)) + 
  geom_histogram(stat="count",bins=3,fill="#015977",color="black")+xlab("Level of H1N1 Knowledge") + geom_text(aes( label = scales::percent(..prop..),y= ..prop.. ), stat= "count", vjust = -.5)
pre_2
```

The histogram shows there to be a suprisingly large proportion of respondents that indicated "A lot of knowledge" of the H1N1 Virus, with 37%. Only 7% indicated "No knowledge". We would expect the majority of those that reported having "A lot of knowledge" to want the vaccine, when compared to those reporting "No knowledge".

0-No knowledge  
1-A little knowledge  
2-A lot of knowledge  

## Histogram for "Behavioral" Variables
```{r}
beh1 <- ggplot(df2, aes(x=behavioral_wash_hands)) + 
  geom_histogram(stat="count",bins=2,fill="#35648F",color="black")+xlab("Frequent Hand Washing / Hand Sanitizer Use") + geom_text(aes( label = scales::percent(..prop..),y= ..prop.. ), stat= "count", vjust = -.5)
beh1
```

17% of respondents indicated infrequent hand washing and lack of sanitizer use, while 83% indicated the opposite. We would expect those that reported frequent usage to be more likely to get the H1N1 vaccine, as this indicates more self-awareness of healthy habits and the presence of germs.

0-No  
1-Yes  

```{r}
beh2 <- ggplot(df2, aes(x=behavioral_large_gatherings)) + 
  geom_histogram(stat="count",bins=2,fill="#35648F",color="black")+xlab("Avoidance of Large Gatherings") + geom_text(aes( label = scales::percent(..prop..),y= ..prop.. ), stat= "count", vjust = -.5)
beh2
```

65% of respondents indicated NOT avoiding large gatherings during the H1N1 crisis, while 35% indicated the opposite. We would expect those that indicated that they practiced avoidance measures to be more likely to get the H1N1 vaccine, as they would seem to be more worried of the risks of the virus.

0-No  
1-Yes  

## Histogram for "Opinion" Variables
```{r}
opi1 <- ggplot(df2, aes(x=opinion_h1n1_risk)) + 
  geom_histogram(stat="count",bins=5,fill="#606DA2",color="black")+xlab("Opinion of H1N1 Risk") + geom_text(aes( label = scales::percent(..prop..),y= ..prop.. ), stat= "count", vjust = -.5)
opi1
```

There is a relatively large disparity between opinions regarding H1N1 risk. Nearly 70% of respondents indicated the risk to be "Very Low" or "Somewhat Low", while only 28.2% indicated the risk posed by H1N1 to be "Somewhat High" or "Very High".  

1-Very Low  
2-Somewhat Low  
3-Don't Know  
4-Somewhat High  
5-Very High  

```{r}
opi2 <- ggplot(df2, aes(x=opinion_h1n1_sick_from_vacc)) + 
  geom_histogram(stat="count",bins=5,fill="#606DA2",color="black")+xlab("Opinion of Sickness from H1N1 Vaccine") + geom_text(aes( label = scales::percent(..prop..),y= ..prop.. ), stat= "count", vjust = -.5)
opi2
```

There is a relatively large disparity between opinions regarding getting sick from the H1N1 vaccince. 69.5% of respondents indicated the risk of sickness to be "Not at all worried" or "Somewhat Worried", while only 30.2% indicated the risk of sickness to be "Somewhat Worried" or "Very Worried".

1-Not at all worried  
2-Not very worried    
3-Don't know  
4-Somewhat worried  
5-Very worried  

## Histogram for "Demographics" Variables
```{r}
dem1 <- ggplot(df2, aes(x=age_group)) + 
  geom_histogram(stat="count",fill="#8C74AE",color="black")+xlab("Age Group")
dem1
```

The 65+ age group was the most represented among respondents, while the 35-44 was the least represented.


```{r}
dem2 <- ggplot(df2, aes(x=education)) + 
  geom_histogram(stat="count",fill="#8C74AE",color="black")+xlab("Level of Education")
dem2
```

The majority of respondents indicated having at least some college. The most represented group were College Graduates.


```{r}
dem3 <- ggplot(df2, aes(x=employment_status)) + 
  geom_histogram(stat="count",fill="#8C74AE",color="black")+xlab("Employment Status")
dem3
```

Over 50% of respondents indicated that they were employed.


## Histogram for "Health Considerations" Variables
```{r}
hc1 <- ggplot(df2, aes(x=doctor_recc_h1n1)) + 
  geom_histogram(stat="count",bins=2,fill="#B77AB0",color="black")+xlab("Doctor Recommendation of H1N1 Vaccine") + geom_text(aes( label = scales::percent(..prop..),y= ..prop.. ), stat= "count", vjust = -.5)
hc1
```

77% of respondents indicated that their doctor did NOT recommend getting the H1N1 Vaccine, while only 23% indicated the presence of a recommendation.

0-No  
1-Yes  

```{r}
hc2 <- ggplot(df2, aes(x=chronic_med_condition)) + 
  geom_histogram(stat="count",bins=2,fill="#B77AB0",color="black")+xlab("Chronic Medical Condition") + geom_text(aes( label = scales::percent(..prop..),y= ..prop.. ), stat= "count", vjust = -.5)
hc2
```

72% or respondents indicated NOT having a chronic medical condition, while 28% indicated the presence of a chronic medical condition.
0-No  
1-Yes  

# Feature Selection {.tabset}

## Corellogram 
```{r, message = FALSE, warnings = FALSE}
h1n1_correlation = ggcorr(df2, method = c("everything", "pearson"), title = "H1N1 Factors Correlogram")
h1n1_correlation
```

A correlogram was utilized to inform and guide future efforts for feature selection. We aimed to identify variables weakly and strongly correlated to the target variable and then test model performance with & without those variables. As demonstrated above, none of the variables are particularly strongly correlated with whether someone relieved the h1n1 vaccination. However, the most correlated are **doctor recommendation**, **opinion about the risk of getting sick with the flu vaccine**, and their **opinion of whether or not the h1n1 vaccine was effective** with correlations of **0.13**, **0.11**, and **0.09** respectively.

Because we determined that no variables were highly correlated, we will not have to remove any of the features and can move on to the LASSO Regression.

## LASSO Regression Model 

### Why LASSO Regression

Initially, we want to utilize lasso regression to reduce the feature space. LASSO regression is an analysis method that utilizes both variable selection and regularization to further enhance the predictive accuracy and interpretability. It imposes a constraint on the model parameters that causes the regression coefficients for some of the variables to shrink towards zero.

### Preparing the Data
```{r}
set.seed(03092000)
library(mltools)
library(data.table)

# Split the data into training and test set
training.samples <- df$h1n1_vaccine %>% 
  createDataPartition(p = 0.8, list = FALSE)
train.data  <- df[training.samples, ]
test.data <- df[-training.samples, ]

#newdata <- one_hot(as.data.table(df))

# Dummy code categorical predictor variables
x <- model.matrix(h1n1_vaccine~., train.data)[,]
# Convert the outcome (class) to a numerical variable
y <- ifelse(train.data$h1n1_vaccine == 1, 1, 0)
```

### Computing Penalized Logistic Regression
```{r}
set.seed(03092000)
library(glmnet)

#Computing the Penalized Logistic Regression
LASSO_Reg <-glmnet(x, y, family = "binomial", alpha = 1) 

#Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, family = "binomial",
                lambda =  LASSO_Reg$lambda.min)

# Make predictions on the test data
x.test <- model.matrix(h1n1_vaccine ~., test.data)[,]
probabilities <- model %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, "pos", "neg")

coef(LASSO_Reg)  #display regression coefficients
plot(LASSO_Reg) #each lines a feature, the ones retained are the ones on the top 
```


```{r}
#getting the actual coefficient for lambda 
coeffs <- coef(LASSO_Reg, s = 0.1) 
coeffs.df <- data.frame(name = coeffs@Dimnames[[1]][coeffs@i + 1], coefficient = coeffs@x) 

# reordering the variables in term of coefficients
coeffs.df[order(coeffs.df$coefficient, decreasing = T),]

#cross-validation done by cv.glmnet
cv.fit <- cv.glmnet(x, y)
cv.fit

plot(cv.fit)

#1SE furthest to the right for lambda 
cv.fit$lambda.min

#Optimized run of the model 
optimal_reg <-glmnet(x, y, family = "binomial", alpha = 0.006919) 
plot(optimal_reg) #look back at the coefficients 
coef(optimal_reg)
```


```{r}
# create a function to transform coefficient of glmnet and cvglmnet to data.frame
coeff2dt <- function(fitobject, s) {
  coeffs <- coef(fitobject, s) 
  coeffs.dt <- data.frame(name = coeffs@Dimnames[[1]][coeffs@i + 1], coefficient = coeffs@x) 

  # reorder the variables in term of coefficients
  return(coeffs.dt[order(coeffs.dt$coefficient, decreasing = T),])
}

coeff2dt(fitobject = cv.fit, s = "lambda.min") %>% head(30)

#threshold - block method (prop.) 
```

# Creating a Lasso Coefficient Plot
```{r}
coeffs.table <- coeff2dt(fitobject = cv.fit, s = "lambda.min")
ggplot(data = coeffs.table) +
  geom_col(aes(x = name, y = coefficient, fill = {coefficient > 0})) +
  xlab(label = "") +
  ggtitle(expression(paste("Lasso Coefficients with ", lambda, " = 0.006919"))) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none") 
```

#Isolate Variables of Interest 
```{r}
varOfInterestPos = coeff2dt(fitobject = cv.fit, s = "lambda.min") %>% filter(coefficient >= 0.03)
varOfInterestNeg = coeff2dt(fitobject = cv.fit, s = "lambda.min") %>% filter(coefficient <= -0.03)
varOfInterest = c( varOfInterestPos[,1], varOfInterestNeg[,1])
varOfInterestFinal = varOfInterest[varOfInterest != "(Intercept)"]
varOfInterestFinal
```
As can be seen above, there are 17 variables that we plan to utilize within our SVM model. This significant reduction of the feature space promotes sparsity and makes the model *"mean and lean"* as described by Professor Wright.

#One-Hot Encoding 
```{r}
library(mltools)
library(Matrix)
#df_onehot = one_hot(df, dropCols = TRUE) FLOP!

df_onehot = as.data.frame(as.matrix(sparse.model.matrix(~. -1, data = df)))

```


```{r}
df_onehot = df_onehot %>% 
  select( c(varOfInterestFinal, h1n1_vaccine1))

names(df_onehot)[18] <- "h1n1_vaccine"

```




# Future Direction for Project

Utilizing both the polynomial kernel and the radial kernel, we will explore tuning the hyperparameters and comparing the performance of each as they both possessed similar RMSE values. We initially ran multiple kernels and selected the Polynomial Kernel due to its low RMSE of 0.401, with the Radial Kernel following close behind at an RSME of 0.417. However, because these kernel’s performed very similarly with regard to our loss function, we would still like to further assess both kernels.

Following the execution of the SVM, we would like to proceed with the Random Forest and aim to tune the hyperparameters for optimal performance. Having tuned both of the models, the statistical significance of the resulting F1 Scores of each algorithm will be assessed for statistical significance with a t-test.


# Creating the Training, Testing, and Validation Sets
```{r}
combined_df <- cbind(df,df_onehot) #28 col in df, 18 col in df_onehot 

# Creating the random training, validation, and testing sets
data_train <- sample(1:nrow(df),
               round(0.7 * nrow(df), 0), 
               replace = FALSE)

data_val <- sample(1:nrow(df),
               round(0.15 * nrow(df), 0), 
               replace = FALSE)

data_test <- sample(1:nrow(df),
               round(0.15 * nrow(df), 0), 
               replace = FALSE)

#Assigning random selection to original df
original_train <- combined_df[data_train, 1:28] #Should contain 70% of data points
original_val <- combined_df[data_val, 1:28]
original_test <- combined_df[data_test, 1:28 ]

#Assigning random selection to one hot encoded df 
onehot_train <- combined_df[data_train, 29:46] #Should contain 70% of data points
onehot_val <- combined_df[data_val, 29:46]
onehot_val <- combined_df[data_test, 29:46]

```

We chose a 70/15/15 split for our analysis, due to a variety of beneficial factors. 70% training data allows for ample examples for our model to find the most accurate solutions during the testing phase. 70% allocates enough “practice” data, while still leaving ample data for use later. In addition, 15% of validation data allows us enough space to choose the best model without giving up more training data. Finally 15% of test data, in our opinion, is the perfect sweet spot to test our model to unseen data. This 15% of test data should be accurate after undergoing the training and validation sets.

# SVM Model {.tabset}


## Creating the First Model - Polynomial Kernel 
```{r}
#install.packages('e1071') 
library(e1071) 

classifier_original <- svm(formula = h1n1_vaccine ~ ., 
                 data = original_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'polynomial')  #The kernel used in training and predicting

```
## Predicting the Test Set Results with Model
```{r}
# Predicting the test set results 

y_pred_original <- predict(classifier_original, newdata = original_test[-28]) 
```

## Creating Confusion Matrix
```{r}
# Making a Confusion Matrix 
#install.packages("caret")
library(caret)
cm <- confusionMatrix(original_test$h1n1_vaccine,y_pred_original, positive = "1")
cm
```
The accuracy and sensitivity were good relative to the basis, at 0.7785 and 0.78205 respectively.

```{r}
f1score_poly = 2*0.78205 *(61/(636+13))/(0.78205  + (61/(636+61)) )
f1score_poly
```
Our primary metric, the F1 Score, was calculated to be 0.169 - this is awful. Let's try it with the radial kernel. 



## Creating the Second Model - Radial Kernel 
```{r}
#install.packages('e1071') 
library(e1071) 

classifier_original2 <- svm(formula = h1n1_vaccine ~ ., 
                 data = original_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial')  #The kernel used in training and predicting

```
## Predicting the Test Set Results with Model
```{r}
# Predicting the test set results 

y_pred_original2 <- predict(classifier_original2, newdata = original_test[-28]) 
```

## Creating Confusion Matrix
```{r}
# Making a Confusion Matrix 
#install.packages("caret")
library(caret)
cm <- confusionMatrix(original_test$h1n1_vaccine,y_pred_original2, positive = "1")
cm
```
The overall accuracy improved some to 0.82, while the false positive rate decreased from 0.2216 to 0.1546. 
```{r}
f1score_radial = 2*0.7303*(306/(391+306))/(0.7303 + (306/(391+306)) )
f1score_radial
```
However, our main metric, the *F1 Score*, is *0.5483*. This is a significant improvement from the the polynomial kernel!
Let's try with the reduced features!

# SVM Model w/ Feature Reduction


## Creating the Model
```{r}
#install.packages('e1071') 
library(e1071) 
set.seed(03092000)

classifier_onehot <- svm(formula = h1n1_vaccine ~ ., 
                 data = onehot_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial')  #The kernel used in training and predicting

```
## Predicting the Test Set Results with Model
```{r}
# Predicting the test set results 
#head(df)
y_pred_onehot <- predict(classifier_onehot, newdata = onehot_test[-18]) 
```

## Creating Confusion Matrix
```{r}
# Making a Confusion Matrix 
#install.packages("caret")
library(caret)
cm <- confusionMatrix(svm_test$h1n1_vaccine,y_pred_onehot, positive = "1")
cm
```

Notably there are improvements in accuracy and sensitivity. The accuracy improved slightly from 0.74 to 0.78 while the sensitivity improved significantly from 0.19 to 0.91. The balanced accuracy improved significantly from 0.48 to 0.85.

```{r}
f1score_radial_onehot = 2*0.22293*(105/(576+105))/(0.22293 + (105/(571+105)) )
f1score_radial_onehot

```
With regard to our key metric, the F1 Score improved from 0.1817 , which is still awful. Now, we will set to tuning the hyperparameters, using the data that did not have a reduced feature space. 



```{r}
library(e1071)
obj <- tune(svm, h1n1_vaccine~., data = original_test, 
            ranges = list(gamma = 2^(-1:1), 
                          cost = 2^(2:4)),
            tunecontrol = tune.control(sampling = "fix"))
summary(obj)

```

```{r}
plot(obj)
```

## Running the Tuned SVM - Radial Kernel  
```{r}
#install.packages('e1071') 
library(e1071) 

classifier_original_tuned <- svm(formula = h1n1_vaccine ~ ., 
                 data = original_train, 
                 type = 'C-classification',  #Default
                 #can change degree 
                 kernel = 'radial',
                 cost = 4,
                 gamma = 0.5)  #The kernel used in training and predicting

```
## Predicting the Test Set Results with Model
```{r}
# Predicting the test set results 

y_pred_original_tuned <- predict(classifier_original_tuned, newdata = original_test[-28]) 
```

## Creating Confusion Matrix
```{r}
# Making a Confusion Matrix 
#install.packages("caret")
library(caret)
cm <- confusionMatrix(original_test$h1n1_vaccine,y_pred_original_tuned, positive = "1")
cm
```


```{r}
f1score_radial_onehot = 2*0.9374 *(158/(539+158))/(0.9374  + (158/(158+539)) )
f1score_radial_onehot

```



# Random Forest Model {.tabset}

## Original Dataset Mtry
```{r}
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}
       
mytry_tune(original_train)
```
Mtry represents the number of variables randomly samples as candidates at each split. Using the mtry_tune function, we find that we should run our initial RF model using mtry = 5.196152, meaning that at each split, five variables will be randomly sampled as candidates at each split.

## Original Dataset Mtry
```{r}
library(randomForest)
set.seed(03092000)

#creating the forest
original_random_forest <- randomForest(h1n1_vaccine ~., 
                          data = original_train,
                          ntree = 200, 
                          mtry = 5,
                          replace = TRUE,
                          importance = TRUE)

original_random_forest #view the forest results
plot(original_random_forest) #plot of the forest results 
```


```{r}
print(importance(original_random_forest, type = 2, scale = TRUE))  #gives the importance of each predictor 
#all the metrics together,not scaled

```
A low Gini or a higher decrease in Gini means that a particular predictor variable plays a greater role in partitioning the data into the defined classes. Therefore, the attribute doctor_recc_h1n1 has the largest mean decrease, meaning it is the most important attribute. Second highest was the opinion_h1n1_risk variable. Lastly, third highest was the opinion_h1n1_vacc_effective variable.


```{r}
View(original_random_forest$err.rate)
```



## Original Random Forest Optimized 
```{r}
library(randomForest)
set.seed(03092000)

#creating the forest
original_random_forest_optimized <- randomForest(h1n1_vaccine ~., 
                          data = original_train,
                          ntree = 133, 
                          mtry = 5,
                          replace = TRUE,
                          importance = TRUE)

original_random_forest_optimized #view the forest results
plot(original_random_forest_optimized) #plot of the forest results 

```

## Comparison of Initial & New Models 
```{r}
original_random_forest$confusion
original_random_forest_optimized$confusion
census_RF_2_acc = sum(original_random_forest_optimized$confusion[row(original_random_forest_optimized$confusion) == 
                                                col(original_random_forest_optimized$confusion)]) / 
  sum(original_random_forest_optimized$confusion)

census_RF_2_acc
```


## Prediction on Training Set 
NOT WORKING 
```{r}
#census_predict = predict(original_random_forest_optimized,      #<- a randomForest model
                    #     original_test,      #<- the test data set to use
                    #     type = "response",   #<- what results to produce, see the help menu for the options
                    #     predict.all = TRUE,  #<- should the predictions of all trees be kept?
                     #    proximity = TRUE)    #<- should proximity measures be computed
#View(census_predict)

```

## Assessment of Model Quality 
NOT WORKING
```{r}
#census_test_pred = data.frame(census_test, 
                         #        Prediction = census_predict$predicted$aggregate)

# View(census_test_pred)

library(caret)
#confusionMatrix(census_test_pred$Prediction, census_test_pred$income,positive = "1", 
     #           dnn=c("Prediction", "Actual"), mode = "everything")
```


## Original Random Forest Optimized - TESTING SET
```{r}
library(randomForest)
set.seed(03092000)

#creating the forest
original_random_forest_testing <- randomForest(h1n1_vaccine ~., 
                          data = original_test,
                          ntree = 200, 
                          mtry = 5,
                          replace = TRUE,
                          importance = TRUE)

original_random_forest_testing #view the forest results
plot(original_random_forest_testing) #plot of the forest results 

```

## One Hot Encoded Dataset Mtry
```{r}
mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}
       
mytry_tune(one)
```
Mtry represents the number of variables randomly samples as candidates at each split. Using the mtry_tune function, we find that we should run our initial RF model using mtry = 5.196152, meaning that at each split, five variables will be randomly sampled as candidates at each split.

## Original Dataset Mtry
```{r}
library(randomForest)
set.seed(03092000)

#creating the forest
original_random_forest <- randomForest(h1n1_vaccine ~., 
                          data = df,
                          ntree = 200, 
                          mtry = 5,
                          replace = TRUE,
                          importance = TRUE)

original_random_forest #view the forest results
plot(original_random_forest) #plot of the forest results 
```


```{r}
print(importance(original_random_forest, type = 2, scale = TRUE))  #gives the importance of each predictor 
#all the metrics together,not scaled

```
A low Gini or a higher decrease in Gini means that a particular predictor variable plays a greater role in partitioning the data into the defined classes. Therefore, the attribute doctor_recc_h1n1 has the largest mean decrease, meaning it is the most important attribute. Second highest was the opinion_h1n1_risk variable. Lastly, third highest was the opinion_h1n1_vacc_effective variable.


```{r}
View(original_random_forest$err.rate)
```



## Original Random Forest Optimized 
```{r}
library(randomForest)
set.seed(03092000)

#creating the forest
original_random_forest_optimized <- randomForest(h1n1_vaccine ~., 
                          data = df,
                          ntree = 177, 
                          mtry = 5,
                          replace = TRUE,
                          importance = TRUE)

original_random_forest_optimized #view the forest results
plot(original_random_forest_optimized) #plot of the forest results 

```


  
# Research and Model Limitations

# Future Analysis
This study primarily focused on developing the best classification model to predict whether a person would receive the H1N1 vaccination or not. However, there are multiple directions that future analysis could go. Below, our group has highlighted the top three future analysis studies we believe would be the strongest and most interesting.

1. This research project focused on specifically the H1N1 (or Swine) Flu epidemic. For future analysis, completing a similar analysis on data collected from other pandemics and epidemics could be critical. A similar analysis could be especially useful when conducting a model build with similar data points and types (such as binary or factor), as well as with similar imbalances in the data. By building models and exploring the topology of the datasets, the data scientist may be able to gain some insight as to what factors make a person more likely to get vaccinated during, or after, a pandemic. They could also use the data to try to predict if a person would get vaccinated in the event of a future pandemic. This is especially important to understand given the current state of the world in regards to the COVID-19 pandemic. It is imperative to note, however, that the COVID-19 pandemic may be somewhat of an “outlier” case. Issues and ideas regarding the vaccine, and even the virus in general, have been hyper-politicized. While the data we examined does not factor into effect political affiliation, that variable would be an important aspect when conducting an analysis on COVID-19. In the future as new viruses emerge, having a clear understanding of the factors that contribute to an individual getting a vaccination will become increasingly important.

More information regarding the hyper-politicization of the COVID-19 pandemic and vaccine can be found [here](https://www.washingtonpost.com/politics/2020/12/22/how-political-leaders-could-persuade-more-americans-get-covid-19-vaccination/).

2. Another approach to a future analysis comes in the form of including the seasonal_vaccine variable, and the other seasonal-specific variables that we originally eliminated from our analysis. We originally didn’t include the seasonal data, as to focus on just H1N1 and how we could predict vaccinations from a “new virus” standpoint. By taking an alternative approach by including both seasonal and H1N1 data, we could examine and model trends between the two vaccines - such as specific demographics that showed opposing views between both vaccines, opinions regarding risks associated between the two viruses/vaccines, etc. This could help give us a better idea as to how people react differently when presented with an unusual and abnormal epidemic, in comparison to the yearly virus that they are all used to. We could also take the data from this different approach and use it as inference into how people may view the COVID-19 pandemic and vaccine. In addition, these varying data and trends between the seasonal and H1N1 data could give health officials ideas in regards to running vaccination campaigns. The data would give us insight into certain groups of people who may see their risk from seasonal to the H1N1 flu to be lesser, and could target these groups for public health and vaccination resources to increase vaccination rates and boost public opinion and knowledge.

3. Lastly, another approach could give us incredible insight regarding some of the variables that we were unable to decode. We were unable to decode the anonymous-code-ids from the hhs_geo_region, employment_industry, and employment_occupation features. In a perfect world, we would have the information regarding how to decode these ids and could use the information in an analysis. These data may be beneficial in building a model that more accurately predicts if someone would get the vaccine, or not. For example, geographical region could have a lot to do with a person’s views on the vaccine - whether that be political, access to health resources, number of virus cases in the area, etc. Occupation and employment industry information may also be important, as some workers in certain sectors may feel that they are at more risk to the virus than others, thus wanting the vaccine.


# References
1. <ftp://ftp.cdc.gov/pub/Health_Statistics/NCHS/Dataset_Documentation/NIS/nhfs/nhfspuf_DUG.PDF>
2. <https://www.cdc.gov/h1n1flu/surveillanceqa.htm>
3. <https://towardsdatascience.com/simplifying-precision-recall-and-other-evaluation-metrics-d066b527c6bb>